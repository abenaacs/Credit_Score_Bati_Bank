{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xverse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder, StandardScaler, MinMaxScaler\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxverse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WOE\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xverse'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from xverse.transformer import WOE\n",
    "from pathlib import Path\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s]: %(message)s\")\n",
    "\n",
    "# Paths to data\n",
    "RAW_DATA_PATH = Path(\"./data/raw/data.csv\")\n",
    "OUTPUT_DATA_PATH = Path(\"./data/processed/processed_data.csv\")\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)\n",
    "        logging.info(f\"Loaded data with shape: {data.shape}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Create aggregate features\n",
    "def create_aggregate_features(data):\n",
    "    logging.info(\"Creating aggregate features...\")\n",
    "    aggregate_features = (\n",
    "        data.groupby(\"AccountId\")\n",
    "        .agg(\n",
    "            Total_Transaction_Amount=(\"Amount\", \"sum\"),\n",
    "            Average_Transaction_Amount=(\"Amount\", \"mean\"),\n",
    "            Transaction_Count=(\"Amount\", \"count\"),\n",
    "            Std_Transaction_Amount=(\"Amount\", \"std\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    data = data.merge(aggregate_features, on=\"AccountId\", how=\"left\")\n",
    "    return data\n",
    "\n",
    "\n",
    "# Extract temporal features\n",
    "def extract_temporal_features(data):\n",
    "    logging.info(\"Extracting temporal features...\")\n",
    "    data[\"TransactionStartTime\"] = pd.to_datetime(\n",
    "        data[\"TransactionStartTime\"], errors=\"coerce\"\n",
    "    )\n",
    "    data[\"Transaction_Hour\"] = data[\"TransactionStartTime\"].dt.hour\n",
    "    data[\"Transaction_Day\"] = data[\"TransactionStartTime\"].dt.day\n",
    "    data[\"Transaction_Month\"] = data[\"TransactionStartTime\"].dt.month\n",
    "    data[\"Transaction_Year\"] = data[\"TransactionStartTime\"].dt.year\n",
    "    return data\n",
    "\n",
    "\n",
    "# Encode categorical variables\n",
    "def encode_categorical_features(data, categorical_columns):\n",
    "    logging.info(\"Encoding categorical variables...\")\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_columns:\n",
    "        data[col] = data[col].astype(str)\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "    return data\n",
    "\n",
    "\n",
    "# Handle missing values\n",
    "def handle_missing_values(data):\n",
    "    logging.info(\"Handling missing values...\")\n",
    "    for col in data.columns:\n",
    "        if data[col].isnull().sum() > 0:\n",
    "            if data[col].dtype in [\"float64\", \"int64\"]:\n",
    "                data[col].fillna(data[col].mean(), inplace=True)\n",
    "            else:\n",
    "                data[col].fillna(\"Unknown\", inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Normalize numerical features\n",
    "def normalize_features(data, numerical_columns):\n",
    "    logging.info(\"Normalizing numerical features...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "    return data\n",
    "\n",
    "\n",
    "# Calculate WoE/IV\n",
    "def calculate_woe_iv(data, target_column, selected_columns):\n",
    "    logging.info(\"Calculating Weight of Evidence (WoE) and Information Value (IV)...\")\n",
    "    woe_transformer = WOE()\n",
    "    data_woe = woe_transformer.fit_transform(\n",
    "        data[selected_columns], data[target_column]\n",
    "    )\n",
    "    iv_values = woe_transformer.woe_values\n",
    "    logging.info(f\"IV values: {iv_values}\")\n",
    "    return data_woe\n",
    "\n",
    "\n",
    "# Main pipeline\n",
    "def feature_engineering_pipeline():\n",
    "    # Step 1: Load data\n",
    "    data = load_data(RAW_DATA_PATH)\n",
    "\n",
    "    # Step 2: Create aggregate features\n",
    "    data = create_aggregate_features(data)\n",
    "\n",
    "    # Step 3: Extract temporal features\n",
    "    data = extract_temporal_features(data)\n",
    "\n",
    "    # Step 4: Encode categorical features\n",
    "    categorical_columns = [\n",
    "        \"CurrencyCode\",\n",
    "        \"CountryCode\",\n",
    "        \"ProviderId\",\n",
    "        \"ProductCategory\",\n",
    "        \"ChannelId\",\n",
    "    ]\n",
    "    data = encode_categorical_features(data, categorical_columns)\n",
    "\n",
    "    # Step 5: Handle missing values\n",
    "    data = handle_missing_values(data)\n",
    "\n",
    "    # Step 6: Normalize numerical features\n",
    "    numerical_columns = [\n",
    "        \"Total_Transaction_Amount\",\n",
    "        \"Average_Transaction_Amount\",\n",
    "        \"Transaction_Count\",\n",
    "        \"Std_Transaction_Amount\",\n",
    "    ]\n",
    "    data = normalize_features(data, numerical_columns)\n",
    "\n",
    "    # Step 7: Calculate WoE/IV\n",
    "    target_column = \"FraudResult\"\n",
    "    selected_columns = [\"ProductCategory\", \"ChannelId\"]\n",
    "    data_woe = calculate_woe_iv(data, target_column, selected_columns)\n",
    "\n",
    "    # Step 8: Save processed data\n",
    "    data.to_csv(OUTPUT_DATA_PATH, index=False)\n",
    "    logging.info(f\"Processed data saved to: {OUTPUT_DATA_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    feature_engineering_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
